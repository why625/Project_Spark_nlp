{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "|bot_id| bot_Desc| bot_Cate| top_id| top_Desc| top_Cate| tag| bot_sub_catA| bot_sub_catB| bot_sub_catC| bot_sub_catD| top_sub_catA|top_sub_catB| top_sub_catC|top_sub_catD|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler,VectorIndexer, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, CountVectorizer, HashingTF, IDF\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel, MultilayerPerceptronClassifier, MultilayerPerceptronClassificationModel, LinearSVC, LinearSVCModel, LogisticRegression, LogisticRegressionModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import lower, length, size\n",
    "from pyspark.sql.functions import rand \n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import split, col\n",
    "from pyspark.ml.feature import OneHotEncoder, OneHotEncoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try with sub cate A\n",
    "data = spark.read.csv(\"hdfs://master.tibame/user/heart/project/dataset/FashionVCdata/pre_dl_dataset_textt.csv\", inferSchema=True, header=True)\n",
    "#data = data2.na.drop() c in \n",
    "\n",
    "#pre-staging: splitting to sub_cate\n",
    "data = data.withColumn(\"bot_sub_catA\", split(col(\"bot_Cate\"), \">\").getItem(1)).withColumn(\"bot_sub_catB\", split(col(\"bot_Cate\"), \">\").getItem(2)).withColumn(\"bot_sub_catC\", split(col(\"bot_Cate\"), \">\").getItem(3)).withColumn(\"bot_sub_catD\", split(col(\"bot_Cate\"), \">\").getItem(4)).withColumn(\"top_sub_catA\", split(col(\"top_Cate\"), \">\").getItem(1)).withColumn(\"top_sub_catB\", split(col(\"top_Cate\"), \">\").getItem(2)).withColumn(\"top_sub_catC\", split(col(\"top_Cate\"), \">\").getItem(3)).withColumn(\"top_sub_catD\", split(col(\"top_Cate\"), \">\").getItem(4)) #.show(2)\n",
    "\n",
    "#data.printSchema()\n",
    "#all-in\n",
    "feature_prep = data.select(lower(data[\"bot_sub_catA\"]).alias(\"bot_sub_catA\"), lower(data[\"bot_sub_catB\"]).alias(\"bot_sub_catB\"),\n",
    " lower(data[\"bot_sub_catC\"]).alias(\"bot_sub_catC\"), lower(data[\"top_sub_catA\"]).alias(\"top_sub_catA\"), lower(data[\"top_sub_catB\"]).alias(\"top_sub_catB\"), lower(data[\"top_sub_catC\"]).alias(\"top_sub_catC\"), lower(data[\"bot_Desc\"]).alias(\"bot_Desc\"), lower(data[\"top_Desc\"]).alias(\"top_Desc\"), (data[\"top_id\"]), (data[\"bot_id\"]), \"tag\")\n",
    "\n",
    "# Preprocessing and feature engineering\n",
    "\n",
    "#Cate 2 Cate\n",
    "stopwordList = [\"null\", \"Null\", '', ' '] \n",
    "stopwordList.extend(StopWordsRemover().getStopWords())\n",
    "stopwordList = list(set(stopwordList))\n",
    "\n",
    "feature_prep = RegexTokenizer(inputCol=\"bot_sub_catA\", outputCol=\"bot_sub_catAwords\", pattern=r\"^[a-z]+$\").transform(feature_prep) #^[a-z]+$\n",
    "feature_prep = StopWordsRemover(inputCol='bot_sub_catAwords',outputCol='bot_sub_catAstop_words_removed', stopWords=stopwordList).transform(feature_prep)\n",
    "feature_prep = HashingTF(inputCol=\"bot_sub_catAstop_words_removed\", outputCol=\"bot_sub_catAtf\", numFeatures=50).transform(feature_prep)\n",
    "\n",
    "feature_prep = RegexTokenizer(inputCol=\"top_sub_catA\", outputCol=\"top_sub_catAwords2\", pattern=r\"^[a-z]+$\").transform(feature_prep) #^[a-z]+$\n",
    "feature_prep = StopWordsRemover(inputCol='top_sub_catAwords2',outputCol='top_sub_catAstop_words_removed2', stopWords=stopwordList).transform(feature_prep)\n",
    "feature_prep = HashingTF(inputCol=\"top_sub_catAstop_words_removed2\", outputCol=\"top_sub_catAtf2\", numFeatures=50).transform(feature_prep)\n",
    "#Desc 2 Desc \n",
    "feature_prep = RegexTokenizer(inputCol=\"bot_Desc\", outputCol=\"words3\", pattern=r\"^[a-z]+$\").transform(feature_prep) #^[a-z]+$\n",
    "feature_prep = StopWordsRemover(inputCol='words3',outputCol='stop_words_removed3', stopWords=stopwordList).transform(feature_prep)\n",
    "feature_prep = HashingTF(inputCol=\"stop_words_removed3\", outputCol=\"tf3\", numFeatures=1000).transform(feature_prep)\n",
    "feature_prep = IDF(inputCol=\"tf3\", outputCol=\"tf_idf3\").fit(feature_prep).transform(feature_prep)\n",
    "\n",
    "feature_prep = RegexTokenizer(inputCol=\"top_Desc\", outputCol=\"words4\", pattern=r\"^[a-z]+$\").transform(feature_prep) #^[a-z]+$\n",
    "feature_prep = StopWordsRemover(inputCol='words4',outputCol='stop_words_removed4', stopWords=stopwordList).transform(feature_prep)\n",
    "feature_prep = HashingTF(inputCol=\"stop_words_removed4\", outputCol=\"tf4\", numFeatures=1000).transform(feature_prep)\n",
    "feature_prep = IDF(inputCol=\"tf4\", outputCol=\"tf_idf4\").fit(feature_prep).transform(feature_prep)\n",
    "\n",
    "feature_prep = StringIndexer(inputCol='tag',outputCol='label_indexed').fit(feature_prep).transform(feature_prep)\n",
    "feature_prep = VectorAssembler(inputCols=[\"top_id\", \"bot_id\",\"bot_sub_catAtf\",\"bot_sub_catBtf\", \"bot_sub_catCtf\", \"top_sub_catAtf2\", \"tf_idf3\", \"tf_idf4\"], outputCol=\"features\", handleInvalid=\"skip\").transform(feature_prep)\n",
    "final_data = feature_prep.select(\"label_indexed\", \"features\")\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_data, test_data = final_data.randomSplit([0.7,0.3])\n",
    "\n",
    "# Model training\n",
    "\n",
    "classifier = LogisticRegression(featuresCol=\"features\", labelCol=\"label_indexed\", regParam=0.1, elasticNetParam=1.0, family=\"multinomial\")\n",
    "classifier = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label_indexed\", numTrees=500, maxDepth=30)\n",
    "# 200 /25\n",
    "# 1000/25\n",
    "\n",
    "model = classifier.fit(train_data)\n",
    "\n",
    "# Transform the test data using the model to get predictions\n",
    "predicted_test_data = model.transform(test_data)\n",
    "\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol='label_indexed', predictionCol='prediction', metricName='accuracy')\n",
    "print(\"Accuracy: {}\", evaluator_accuracy.evaluate(predicted_test_data)) \n",
    "\n",
    "# Save the model\n",
    "model.save('hdfs://master.tibame/user/heart/project/dataset/FashionVCdata/ALLIN_classifier_model')\n"
   ]
  }
 ]
}
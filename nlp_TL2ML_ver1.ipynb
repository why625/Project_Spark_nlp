{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler,VectorIndexer, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, CountVectorizer, HashingTF, IDF\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel, MultilayerPerceptronClassifier, MultilayerPerceptronClassificationModel, LinearSVC, LinearSVCModel, LogisticRegression, LogisticRegressionModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import lower, length, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_top_pre = spark.read.csv(\"hdfs://master.tibame/user/heart/project/dataset/FashionVCdata/top.csv\", inferSchema=True, header=True)\n",
    "data_top = data_top_pre.na.drop()\n",
    "#res1 = data_top.withColumn('id', sparkf.monotonically_increasing_id())\n",
    "\n",
    "data_bott_pre = spark.read.csv(\"hdfs://master.tibame/user/heart/project/dataset/FashionVCdata/bottom.csv\", inferSchema=True, header=True) #13662\n",
    "data_bott = data_bott_pre.na.drop()\n",
    "#data_bott.show(truncate=False)\n",
    "#res11 = data_bott.withColumn('id', sparkf.monotonically_increasing_id())\n",
    "\n",
    "data_gnd_pre = spark.read.csv(\"hdfs://master.tibame/user/heart/project/dataset/FashionVCdata/gnd_top_bottom_pairs.csv\", inferSchema=True, header=False)\n",
    "data_gnd = data_gnd_pre.na.drop()\n",
    "\n",
    "data_top.show()\n",
    "data_bott.show()\n",
    "data_gnd.show()\n",
    "joined_bott_top_aswr\n",
    "joined_bott_top_ques\n",
    "pre_dl_dataset\n",
    "(\"./pre_dl_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL\n",
    "# Prepare data\n",
    "data = spark.read.csv(\"hdfs://master.tibame/user/heart/project/dataset/FashionVCdata/pre_dl_dataset_textt.csv\",inferSchema=True, header=True)\n",
    "\n",
    "feature_prep = data.select(lower(data[\"bot_Cate\"]).alias(\"bot_Cate\"), lower(data[\"top_Cate\"]).alias(\"top_Cate\"),lower(data[\"bot_Desc\"]).alias(\"bot_Desc\"), lower(data[\"top_Desc\"]).alias(\"top_Desc\"), (data[\"top_id\"]), (data[\"bot_id\"]), \"tag\")\n",
    "\n",
    "# Preprocessing and feature engineering\n",
    "#Cate 2 Cate\n",
    "feature_prep = RegexTokenizer(inputCol=\"bot_Cate\", outputCol=\"words\", pattern=r\"^[a-z]+$\").transform(feature_prep) #^[a-z]+$\n",
    "\n",
    "stopwordList = [\"null\", \"Null\", '', ' '] \n",
    "stopwordList.extend(StopWordsRemover().getStopWords())\n",
    "stopwordList = list(set(stopwordList))\n",
    "\n",
    "feature_prep = StopWordsRemover(inputCol='words',outputCol='stop_words_removed', stopWords=stopwordList).transform(feature_prep)\n",
    "feature_prep = HashingTF(inputCol=\"stop_words_removed\", outputCol=\"tf\", numFeatures=5000).transform(feature_prep)\n",
    "feature_prep = IDF(inputCol=\"tf\", outputCol=\"tf_idf\").fit(feature_prep).transform(feature_prep)\n",
    "\n",
    "feature_prep = RegexTokenizer(inputCol=\"top_Cate\", outputCol=\"words2\", pattern=r\"^[a-z]+$\").transform(feature_prep) \n",
    "feature_prep = StopWordsRemover(inputCol='words2',outputCol='stop_words_removed2', stopWords=stopwordList).transform(feature_prep)\n",
    "feature_prep = HashingTF(inputCol=\"stop_words_removed2\", outputCol=\"tf2\", numFeatures=5000).transform(feature_prep)\n",
    "feature_prep = IDF(inputCol=\"tf2\", outputCol=\"tf_idf2\").fit(feature_prep).transform(feature_prep)\n",
    "#Desc 2 Desc \n",
    "\n",
    "feature_prep = RegexTokenizer(inputCol=\"bot_Desc\", outputCol=\"words3\", pattern=r\"^[a-z]+$\").transform(feature_prep) #^[a-z]+$\n",
    "feature_prep = StopWordsRemover(inputCol='words3',outputCol='stop_words_removed3', stopWords=stopwordList).transform(feature_prep)\n",
    "feature_prep = HashingTF(inputCol=\"stop_words_removed3\", outputCol=\"tf3\", numFeatures=5000).transform(feature_prep)\n",
    "feature_prep = IDF(inputCol=\"tf3\", outputCol=\"tf_idf3\").fit(feature_prep).transform(feature_prep)\n",
    "\n",
    "feature_prep = RegexTokenizer(inputCol=\"top_Desc\", outputCol=\"words4\", pattern=r\"^[a-z]+$\").transform(feature_prep) #^[a-z]+$\n",
    "feature_prep = StopWordsRemover(inputCol='words4',outputCol='stop_words_removed4', stopWords=stopwordList).transform(feature_prep)\n",
    "feature_prep = HashingTF(inputCol=\"stop_words_removed4\", outputCol=\"tf4\", numFeatures=5000).transform(feature_prep)\n",
    "feature_prep = IDF(inputCol=\"tf4\", outputCol=\"tf_idf4\").fit(feature_prep).transform(feature_prep)\n",
    "\n",
    "\n",
    "feature_prep = StringIndexer(inputCol='tag',outputCol='label_indexed').fit(feature_prep).transform(feature_prep)\n",
    "feature_prep = VectorAssembler(inputCols=[\"top_id\", \"bot_id\",\"tf_idf\", \"tf_idf2\",\"tf_idf3\", \"tf_idf4\"], outputCol=\"features\", handleInvalid=\"skip\").transform(feature_prep)\n",
    "final_data = feature_prep.select(\"label_indexed\", \"features\")\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_data, test_data = final_data.randomSplit([0.7,0.3])\n",
    "\n",
    "\n",
    "# Model training\n",
    "classifier = LogisticRegression(featuresCol=\"features\", labelCol=\"label_indexed\", regParam=0.1, elasticNetParam=1.0, family=\"multinomial\")\n",
    "classifier = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label_indexed\", numTrees=500, maxDepth=30)\n",
    "# 200 /25\n",
    "# 1000/25\n",
    "model = classifier.fit(train_data)\n",
    "\n",
    "# Transform the test data using the model to get predictions\n",
    "predicted_test_data = model.transform(test_data)\n",
    "\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol='label_indexed', predictionCol='prediction', metricName='accuracy')\n",
    "print(\"Accuracy: {}\", evaluator_accuracy.evaluate(predicted_test_data)) \n",
    "\n",
    "# Save the model\n",
    "model.save('hdfs://master.tibame/user/heart/project/dataset/FashionVCdata/ALLIN_classifier_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cate 2 Cate\n",
    "# Prepare data\n",
    "data = spark.read.csv(\"hdfs://master.tibame/user/heart/project/dataset/FashionVCdata/pre_dl_dataset_text.csv\",inferSchema=True, header=True)\n",
    "# Preprocessing and feature engineering\n",
    "feature_prep = data.select(lower(data[\"bot_Cate\"]).alias(\"bot_Cate\"), lower(data[\"top_Cate\"]).alias(\"top_Cate\"), \"tag\")\n",
    "\n",
    "feature_prep = RegexTokenizer(inputCol=\"bot_Cate\", outputCol=\"words\", pattern=r\"^[a-z]+$\").transform(feature_prep) #^[a-z]+$\n",
    "\n",
    "stopwordList = [\"null\", \"Null\", '', ' '] \n",
    "stopwordList.extend(StopWordsRemover().getStopWords())\n",
    "stopwordList = list(set(stopwordList))\n",
    "\n",
    "feature_prep = StopWordsRemover(inputCol='words',outputCol='stop_words_removed', stopWords=stopwordList).transform(feature_prep)\n",
    "feature_prep = HashingTF(inputCol=\"stop_words_removed\", outputCol=\"tf\", numFeatures=5000).transform(feature_prep)\n",
    "feature_prep = IDF(inputCol=\"tf\", outputCol=\"tf_idf\").fit(feature_prep).transform(feature_prep)\n",
    "\n",
    "\n",
    "feature_prep = RegexTokenizer(inputCol=\"top_Cate\", outputCol=\"words2\", pattern=r\"^[a-z]+$\").transform(feature_prep) #^[a-z]+$\n",
    "feature_prep = StopWordsRemover(inputCol='words2',outputCol='stop_words_removed2', stopWords=stopwordList).transform(feature_prep)\n",
    "feature_prep = HashingTF(inputCol=\"stop_words_removed2\", outputCol=\"tf2\", numFeatures=5000).transform(feature_prep)\n",
    "feature_prep = IDF(inputCol=\"tf2\", outputCol=\"tf_idf2\").fit(feature_prep).transform(feature_prep)\n",
    "feature_prep = StringIndexer(inputCol='tag',outputCol='label_indexed').fit(feature_prep).transform(feature_prep)\n",
    "feature_prep = VectorAssembler(inputCols=[\"tf_idf\", \"tf_idf2\"], outputCol=\"features\").transform(feature_prep)\n",
    "\n",
    "final_data = feature_prep.select(\"label_indexed\", \"features\")\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_data, test_data = final_data.randomSplit([0.7,0.3])\n",
    "\n",
    "#==============================================================================\n",
    "# Model training'''\n",
    "classifier = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label_indexed\", numTrees=500, maxDepth=30)\n",
    "# 200 /25\n",
    "classifier = LogisticRegression(featuresCol=\"features\", labelCol=\"label_indexed\", regParam=0.1, elasticNetParam=1.0, family=\"multinomial\")\n",
    "\n",
    "#pyspark.ml.classification.NaiveBayes(featuresCol='features', labelCol='label', predictionCol='prediction', probabilityCol='probability', rawPredictionCol='rawPrediction', smoothing=1.0, modelType='multinomial', thresholds=None, weightCol=None)\n",
    "\n",
    "model = classifier.fit(train_data)\n",
    "\n",
    "\n",
    "# Transform the test data using the model to get predictions\n",
    "predicted_test_data = model.transform(test_data)\n",
    "\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol='label_indexed', predictionCol='prediction', metricName='accuracy')\n",
    "print(\"Accuracy: {}\", evaluator_accuracy.evaluate(predicted_test_data)) \n",
    "\n",
    "# Save the model\n",
    "model.save('hdfs://master.tibame/user/heart/project/dataset/FashionVCdata/Cate_classifier_model_LR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Desc 2 Desc\n",
    "# Prepare data\n",
    "data = spark.read.csv(\"hdfs://master.tibame/user/heart/project/dataset/FashionVCdata/pre_dl_dataset_text.csv\",inferSchema=True, header=True)\n",
    "# Preprocessing and feature engineering\n",
    "feature_prep = data.select(lower(data[\"bot_Desc\"]).alias(\"bot_Desc\"), lower(data[\"top_Desc\"]).alias(\"top_Desc\"), \"tag\")\n",
    "\n",
    "feature_prep = RegexTokenizer(inputCol=\"bot_Desc\", outputCol=\"words\", pattern=r\"^[a-z]+$\").transform(feature_prep) #^[a-z]+$\n",
    "\n",
    "stopwordList = [\"null\", \"Null\", '', ' '] \n",
    "stopwordList.extend(StopWordsRemover().getStopWords())\n",
    "stopwordList = list(set(stopwordList))\n",
    "\n",
    "feature_prep = StopWordsRemover(inputCol='words',outputCol='stop_words_removed', stopWords=stopwordList).transform(feature_prep)\n",
    "feature_prep = HashingTF(inputCol=\"stop_words_removed\", outputCol=\"tf\", numFeatures=5000).transform(feature_prep)\n",
    "feature_prep = IDF(inputCol=\"tf\", outputCol=\"tf_idf\").fit(feature_prep).transform(feature_prep)\n",
    "\n",
    "feature_prep = RegexTokenizer(inputCol=\"top_Desc\", outputCol=\"words2\", pattern=r\"^[a-z]+$\").transform(feature_prep) #^[a-z]+$\n",
    "feature_prep = StopWordsRemover(inputCol='words2',outputCol='stop_words_removed2', stopWords=stopwordList).transform(feature_prep)\n",
    "feature_prep = HashingTF(inputCol=\"stop_words_removed2\", outputCol=\"tf2\", numFeatures=5000).transform(feature_prep)\n",
    "feature_prep = IDF(inputCol=\"tf2\", outputCol=\"tf_idf2\").fit(feature_prep).transform(feature_prep)\n",
    "\n",
    "feature_prep = StringIndexer(inputCol='tag',outputCol='label_indexed').fit(feature_prep).transform(feature_prep)\n",
    "feature_prep = VectorAssembler(inputCols=[\"tf_idf\", \"tf_idf2\"], outputCol=\"features\").transform(feature_prep)\n",
    "final_data = feature_prep.select(\"label_indexed\", \"features\")\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_data, test_data = final_data.randomSplit([0.7,0.3])\n",
    "\n",
    "# Model training\n",
    "classifier = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label_indexed\", numTrees=800, maxDepth=25)\n",
    "#500 / 30 \n",
    "#1000/25\n",
    "classifier = LogisticRegression(featuresCol=\"features\", labelCol=\"label_indexed\", regParam=0.1, elasticNetParam=1.0, family=\"multinomial\")\n",
    "\n",
    "model = classifier.fit(train_data)\n",
    "\n",
    "# Transform the test data using the model to get predictions\n",
    "predicted_test_data = model.transform(test_data)\n",
    "\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol='label_indexed', predictionCol='prediction', metricName='accuracy')\n",
    "print(\"Accuracy: {}\", evaluator_accuracy.evaluate(predicted_test_data)) \n",
    "\n",
    "# Save the model\n",
    "model.save(\"hdfs://devenv/user/spark/spark_mllib_101/spam_detection/Desc_classifier_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID 2 ID\n",
    "# Prepare data\n",
    "data = spark.read.csv(\"hdfs://master.tibame/user/heart/project/dataset/FashionVCdata/pre_dl_dataset_textt.csv\",inferSchema=True, header=True)\n",
    "# Preprocessing and feature engineering\n",
    "feature_prep = data.select((data[\"top_id\"]), (data[\"bot_id\"]), \"tag\")\n",
    "feature_prep = VectorAssembler(inputCols=[\"top_id\", \"bot_id\"], outputCol=\"features\", handleInvalid=\"skip\").transform(feature_prep)\n",
    "final_data = feature_prep.select(\"tag\", \"features\")\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_data, test_data = final_data.randomSplit([0.7,0.3])\n",
    "\n",
    "# Model training\n",
    "classifier = LogisticRegression(featuresCol=\"features\", labelCol=\"tag\", regParam=0.1, elasticNetParam=1.0)\n",
    "#classifier = RandomForestClassifier(featuresCol=\"features\", labelCol=\"tag\", numTrees=200, maxDepth=25)\n",
    "\n",
    "\n",
    "model = classifier.fit(train_data)\n",
    "# Transform the test data using the model to get predictions\n",
    "predicted_test_data = model.transform(test_data)\n",
    "\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol='tag', predictionCol='prediction', metricName='accuracy')\n",
    "print(\"Accuracy: {}\", evaluator_accuracy.evaluate(predicted_test_data)) \n",
    "\n",
    "# Save the model\n",
    "model.save(\"hdfs://master.tibame/user/heart/project/dataset/FashionVCdata/id_classifier_model\")"
   ]
  }
 ]
}
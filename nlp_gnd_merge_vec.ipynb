{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#統整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, IDF, CountVectorizer\n",
    "from pyspark.sql.functions import rand \n",
    "from pyspark.sql.functions import lit\n",
    "import pyspark.sql.functions as sparkf\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .getOrCreate()     \n",
    "''' '''\n",
    "data_gnd 答案 joined_bott_top_aswr tag=1\n",
    "rand_gnd 問題 joined_bott_top_ques tag=0 '''\n",
    "\n",
    "#Aset\n",
    "data_gnd_pre = spark.read.csv(\"hdfs://master.tibame/user/heart/project/dataset/FashionVCdata/gnd_top_bottom_pairs.csv\", inferSchema=True, header=False)\n",
    "data_gnd = data_gnd_pre.na.drop()\n",
    "\n",
    "joined_bott = data_gnd.join(ready_data_bott, data_gnd._c1 == ready_data_bott.bot_id, 'inner').select('*')\n",
    "joined_bott_top_aswr = joined_bott.join(ready_data_top, joined_bott._c0 == ready_data_top.top_id, 'inner' ).select('bot_id', 'tf_idf_b', 'tf_idf_c', 'top_id', 'tf_idf_b_top', 'tf_idf_c_top')\n",
    "joined_bott_top_aswr = joined_bott_top_aswr.withColumn('tag', lit(1))#.show(10) #.count() #20655\n",
    "\n",
    "#Qset\n",
    "w = Window().orderBy(F.lit(\"rand\"))\n",
    "bott_id_rand = data_gnd.select('_c0').withColumn('rand', F.row_number().over(w)).orderBy('rand')\n",
    "top_id_rand = data_gnd.select('_c1').withColumn('pre_rand', sparkf.monotonically_increasing_id()).orderBy(rand())\n",
    "top_id_rand_2 = top_id_rand.select('_c1').withColumn('rand', F.row_number().over(w)).orderBy('rand')\n",
    "rand_gnd = bott_id_rand.join(top_id_rand_2, bott_id_rand.rand == top_id_rand_2.rand ).select('_c0', '_c1')\n",
    "\n",
    "joined_bott2 = rand_gnd.join(ready_data_bott, data_gnd._c1 == ready_data_bott.bot_id).select('*')\n",
    "joined_bott_top_ques = joined_bott2.join(ready_data_top, joined_bott._c0 == ready_data_top.top_id, 'inner' ).select('bot_id', 'tf_idf_b', 'tf_idf_c', 'top_id', 'tf_idf_b_top', 'tf_idf_c_top')\n",
    "joined_bott_top_ques = joined_bott_top_ques.withColumn('tag', lit(0))#.show(10)  #.count() #20655\n",
    "\n",
    "#Q&A set\n",
    "pre_dl_dataset = joined_bott_top_ques.union(joined_bott_top_aswr)\n",
    "pre_dl_dataset.show()\n",
    "pre_dl_dataset.count() #41310\n",
    "\n",
    "#存起來\n",
    "\n",
    "pandasDF2 = pre_dl_dataset.toPandas()\n",
    "pandasDF2.to_csv(\"./pre_dl_dataset.csv\") \n",
    "#pandasDF2.head(10) #.count()\n",
    "\n",
    "# +---------+--------------------+--------------------+---------+--------------------+--------------------+---+\n",
    "# |   bot_id|            tf_idf_b|            tf_idf_c|   top_id|        tf_idf_b_top|        tf_idf_c_top|tag|\n",
    "# +---------+--------------------+--------------------+---------+--------------------+--------------------+---+\n",
    "# |191564594|(6437,[2,4,53,164...|(1738,[0,1,5,12,5...| 47549965|(7041,[296,1590,3...|(1859,[0,1,3,5,7]...|  0|\n",
    "# |195060772|(6437,[2,11,36,37...|(1738,[0,1,5,22,3...| 95985283|(7041,[4,165,880,...|(1859,[0,1,2,4,29...|  0|\n",
    "# |190366907|(6437,[0,39,62,63...|(1738,[0,1,2,21,2...| 96517114|(7041,[15,64,65,7...|(1859,[0,1,2,4,18...|  0|\n",
    "# |157180593|(6437,[0,133,170]...|(1738,[0,1,2,7,8,...|100745635|(7041,[30,48,190,...|(1859,[0,1,2,13,1...|  0|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data -Preview\n",
    "#bott\n",
    "data_bott = spark.read.csv(\"hdfs://master.tibame/user/heart/project/dataset/FashionVCdata/bottom.csv\", inferSchema=True, header=True)\n",
    "\n",
    "data_bott.show(100,truncate=False)\n",
    "data_bott.distinct()\n",
    "data_bott.distinct().count() #13662\n",
    "\n",
    "#top\n",
    "data_top = spark.read.csv(\"hdfs://master.tibame/user/heart/project/dataset/FashionVCdata/top.csv\", inferSchema=True, header=True)\n",
    "\n",
    "data_top.show(100, False)\n",
    "data_top.distinct()\n",
    "data_top.distinct().count() #14871\n",
    "\n",
    "# .....nlp_feature_engineering"
   ]
  },
  {
   "source": [
    "#ver2 addition\n",
    "\n",
    "#make tag\n",
    "\n",
    "#title split with - into 2 and create columns    as  brand & product desc\n",
    "words_added = Tokenizer(inputCol=\"message\", outputCol=\"words\").transform(df)\n",
    "words_added.show(truncate=False)\n",
    "#category split with > into 4 and create columns  \n",
    "#ttl 7 col\n",
    "\n",
    "header= [\"PID\",\"Brand\", \"Desc\", \"Cate1\", \"Cate2\", \"Cate3\", \"Cate4\"] \n",
    "\n",
    "-----------------------------------------------------\n",
    "id_1 | id_2 | Desc(tf-idf) | cate1(string[1.2.3.4...]]) | cate2(string) | cate3(string) | cate4(string)  | ..id2's'......... | y = [1, 0, 1, 0,0,0,0,0...]\n",
    "------------------------------------------------------\n",
    "\n",
    "#nlp set\n",
    "# 1 + 2 > 3  = current hot  X\n",
    "# 1 + 2 > 4  = core product X\n",
    "# 3 + 4+5+6+7  > 1\n",
    "# 3 + 4/5/6/7  > 1    100D:based on gnd\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}